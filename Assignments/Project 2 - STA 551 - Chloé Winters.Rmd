---
title: "Supervised Programing for Customer Churn"
author: "Chloé Winters"
date: "2025-11-06"
output:
  html_document:           # output document format
    toc: yes               # add table contents
    toc_float: yes         # toc_property: floating
    toc_depth: 4           # depth of TOC headings
    fig_width: 6           # global figure width
    fig_height: 4          # global figure height
    fig_caption: yes       # add figure caption
    number_sections: yes   # numbering section headings
    toc_collapsed: yes     # TOC subheading clapsing
    code_folding: hide     # folding/showing code 
    code_download: yes     # allow to download complete RMarkdown source code
    smooth_scroll: yes     # scrolling text of the document
    theme: lumen           # visual theme for HTML document only
    highlight: tango       # code syntax hightlighting styles
  pdf_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    number_sections: yes
  word_document:
    toc: yes
    toc_depth: '4'
---

```{css, echo = FALSE}
div#TOC li {     /* table of content  */
    list-style:upper-roman;
    background-image:none;
    background-repeat:none;
    background-position:0;
}

h1.title {    /* level 1 header of title  */
  font-size: 24px;
  font-weight: bold;
  color: DarkRed;
  text-align: center;
}

h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  font-weight: bold;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}

h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  font-weight: bold;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}

h1 { /* Header 1 - and the author and data headers use this too  */
    font-size: 20px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: center;
}

h2 { /* Header 2 - and the author and data headers use this too  */
    font-size: 18px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { /* Header 3 - and the author and data headers use this too  */
    font-size: 16px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - and the author and data headers use this too  */
    font-size: 14px;
  font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

/* Add dots after numbered headers */
.header-section-number::after {
  content: ".";
}
```




```{r setup, include=FALSE}
# code chunk specifies whether the R code, warnings, and output 
# will be included in the output files.
library(knitr)
knitr::opts_chunk$set(echo = TRUE,           # include code chunk in the output file
                      warnings = FALSE,       # sometimes, you code may produce warning messages,
                                              # you can choose to include the warning messages in
                                              # the output file. 
                      results = TRUE          # you can also decide whether to include the output
                                              # in the output file.
                      )   



library(car)
library(caret)
library(corrplot)
library(ggcorrplot)
library(GGally)
library(ggplot2)
library(gridExtra)
library(knitr)
library(lubridate)
library(MASS)
library(mlbench)
library(pander)
library(pROC)
library(tidyverse)
library(vcd)
library(reshape2)
library(neuralnet)
library(gridExtra)


if (!require("ggplot2")) {
   install.packages("ggplot2")
   library(ggplot2)
}
if (!require("ISLR")) {
   install.packages("ISLR")
   library(ISLR)
}
if (!require("knitr")) {
   install.packages("knitr")
   library(knitr)
}
if (!require("ISLR")) {
   install.packages("ISLR")
   library(ISLR)
}
if (!require("pander")) {
   install.packages("pander")
   library(pander)
}
if (!require("rpart")) {
   install.packages("rpart")
   library(rpart)
}
if (!require("rattle")) {
   install.packages("rattle")
   library(rattle)
}
if (!require("rpart.plot")) {
   install.packages("rpart.plot")
   library(rpart.plot)
}
if (!require("rpart.plot")) {
   install.packages("rpart.plot")
   library(rpart.plot)
}
if (!require("RColorBrewer")) {
   install.packages("RColorBrewer")
   library(RColorBrewer)
}
if (!require("e1071")) {
   install.packages("e1071")
   library(e1071)
}
if (!require("glmnet")) {
   install.packages("glmnet")
   library(glmnet)
}
if (!require("pROC")) {
   install.packages("pROC")
   library(pROC)
}

# knitr::opts_knit$set(root.dir = "C:/Users/75CPENG/OneDrive - West Chester University of PA/Documents")
# knitr::opts_knit$set(root.dir = "C:\\STA490\\w05")

knitr::opts_chunk$set(echo = TRUE,       
                      warning = FALSE,   
                      result = TRUE,   
                      message = FALSE,
                      comment = NA) 

```



# Introduction

This assignment will look at customer churn data from several telecommunication companies. The goal is to determine what makes a customer more likely or less likely to churn so these companies can understand how to best retain customers. Supervised programming techniques qill be utelized in the model building process, followed by ROC and Bagging AUC analysis to ultimately determine the final model. 

# Materials 

## Data Set

This data set is a small portion of a telecommunication companies customer data. The data set was created to investigate service quality and improve customer retention. There are 1,000 observations in the data set and 14 variables which are described below. While the categorical variables do get recategorized as factor variables for analysis purposes we will describe the data set as it originally stands. 

```{r}
url ="https://chloewinters79.github.io/STA551/Data/churn_output.csv"
churn = read.csv(url, header = TRUE)

cat_vars <- c("Sex", "Marital_Status", "Phone_service", "International_plan",
              "Voice_mail_plan", "Multiple_line", "Internet_service",
              "Technical_support", "Streaming_Videos", "Agreement_period", "Churn")

churn[cat_vars] <- lapply(churn[cat_vars], factor)

```


1. Sex: (cat) customer’s gender
2. Marital_Status: (cat) marital status of the customer
3. Term: (num) duration of the customer’s term in months
4. Phone_service: (cat) indicates whether the customer has phone service (Yes = has phone service; No = does not have phone service)
5. International_plan: (cat) indicates whether the customer has an international calling plan (Yes = has international plan; No = does not have international plan)
6. Voice_mail_plan: (cat) indicates whether the customer has a voice mail plan (Yes = has voice mail plan; No = does not have voice mail plan)
7. Multiple_line: (cat) indicates whether the customer has multiple phone lines (Yes = has multiple lines; No = does not have multiple lines; No phone = does not have phone service)
8. Internet_service: (cat) type of internet service the customer has (Cable, Fibre optic, DSL, or No internet)
9. Technical_support: (cat) indicates whether the customer has technical support (Yes = has technical support; No = does not have technical support; No internet = does not have internet service)
10. Streaming_Videos: (cat) indicates whether the customer has streaming video service (Yes = has streaming service; No = does not have streaming service; No internet = does not have internet service)
11. Agreement_period: (cat) type of contract agreement (Monthly, One year, or Two year contract)
12. Monthly_Charges: (num) monthly service charges billed to the customer
13. Total_Charges: (num) total charges incurred by the customer over their service term
14. Churn: (cat) indicates customer churn status (Yes = customer churned; No = customer retained)

## Exploratory Data Analysis

Now that we are familiar with our starting data set we need to go through EDA and data cleaning so the final working data set meets the requirements of the assignment. 


First we start by checking for missing values, as shown in the output none of the variables have any missing observations which means we do not need to do any imputation or removing of observations for too many missing values. 

```{r}
# Check for Missing Values
colSums(is.na(churn))
```

The next step is to make sure all the variables are properly categorized for our analysis, so the approportiate variables are being converted to factor variables to make for an easier analysis of these variables when we go into our model building. 


```{r}
# Convert Appropriate Variables to Factors
cat_vars <- c("Sex", "Marital_Status", "Phone_service", "International_plan",
              "Voice_mail_plan", "Multiple_line", "Internet_service",
              "Technical_support", "Streaming_Videos", "Agreement_period", "Churn")

churn[cat_vars] <- lapply(churn[cat_vars], factor)


```

Now moving into observing the summary statistics this output is actually very important, becasue the original output showed that the International plan variable had "yes" and "Yes" observations which were being split so we were able to write code that combined these two types of observations since they are the same.


Looking at the rest of the summary statistics, Term, Monthly_Charges, and Total_Charges all appear to be right-skewed, indicating that most customers have shorter service terms, lower monthly bills, and lower total charges, with a smaller number of customers showing much higher values. This non-normality  could affect models like logistic regression and perceptron, which are sensitive to scale and distribution. Applying standardization or log transformation (particularly for Total_Charges) could help stabilize variance and improve model performance. The categorical variables look well-balanced overall, though the Churn variable shows a moderate imbalance (about 26% churned), which should be monitored to ensure fair model evaluation. Overall, most variables are suitable for modeling after basic preprocessing and scaling.


```{r}
# Summary Statistics 
# For categorical variables
for (v in cat_vars) {
  print(v)
  print(table(churn[[v]]))
}

# For numerical variables
num_vars <- c("Term", "Monthly_Charges", "Total_Charges")
summary(churn[num_vars])


# Fix International
churn$International_plan <- ifelse(tolower(churn$International_plan) == "yes", "Yes",
                            ifelse(tolower(churn$International_plan) == "no", "No",
                                   churn$International_plan))

churn$International_plan <- factor(churn$International_plan)
```

Looking at our binary variable churn, we see a skew towards No, which means a majority of customers are not churning which is ideal for the company. Considering the size of the data set we are working with the lower amount of yes for churns is not concering because despite being almost a 3:1 ratio of no to yes we still have plenty of data to work with and we should not be concerened with this impacting our analysis. 

```{r}
# Explore Target Variable (Churn) 
prop.table(table(churn$Churn))
ggplot(churn, aes(x = Churn, fill = Churn)) +
  geom_bar() +
  labs(title = "Distribution of Customer Churn") +
  theme_minimal()
```

Looking at the relationship between our feature variables and the churn is very interesting for the most part with categorical variables the yes and no for churn match pretty well, there doesn't seem to be any difference between the different factor levels. The only time this is not true is typically when we have factor levels that contain a "no internet option" these levels have much lower yes observations compared to their other factors. Additionally in the Agreement Period variable we see less churn the longer the agreement is for. Looking at the numerical variables is where we see some change. We see that customers who do churn have less terms and higher monthly charges compared to those who don't churn, however the total charges for churners vs non churners is not too different comparatively. 

```{r}
# Explore Relationships Between Features and Churn

# Numeric vs Target
churn %>%
  dplyr::select(all_of(num_vars), Churn) %>%
  pivot_longer(cols = all_of(num_vars), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Churn, y = Value, fill = Churn)) +
  geom_boxplot(outlier.color = "red") +
  facet_wrap(~Variable, scales = "free") +
  labs(title = "Numeric Feature Distributions by Churn Status") +
  theme_minimal()

```

```{r}

# Select categorical variables excluding Churn
cat_vars_plot <- cat_vars[-length(cat_vars)]

# Set up 3x3 grid
par(mfrow = c(2, 3), mar = c(5, 4, 2, 1))

for (v in cat_vars_plot) {
  counts <- table(churn[[v]], churn$Churn)
  prop <- prop.table(counts, 1)  # row proportions
  barplot(t(prop), col = c("blue", "red"), beside = TRUE,
          main = v, ylab = "Proportion", legend.text = TRUE)
}

# Reset layout
par(mfrow = c(1, 1))

```

Looking at the correlation between the numerical variables there seems to be high correlation between term and total charges, and moderate correaltion between total charges and monthly charges. These do make sense in the context of the data since the longer a customer is with the company the more terms they will have and thus the more they will have paid overall. So this correlation while high does make sense contextually and does not appear to me a major cause for concern at this moment just something to keep in mind. 


```{r}

num_data <- churn %>%
  select(all_of(num_vars)) %>%
  drop_na()


# Compute correlation matrix
cor_mat <- cor(num_data)

# Melt correlation matrix for ggplot
cor_melt <- melt(cor_mat)

# Heatmap
ggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1,1), space = "Lab",
                       name="Correlation") +
  geom_text(aes(label = round(value, 2)), color = "black", size = 4) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Correlation Heatmap of Numeric Variables", x = "", y = "")
```

Now that all the exploratory data analysis has been conducted and there does not appear to be anything drastically concerning that needs to undergo specific transformations in this moment we can move onto our analysis and starting with our question.


# Methodolgy & Analysis 

In this assignment we are going to be looking into which variables help predict the likelihood of a customer churning. This assignment will cover three different types of models, logistic, perceptron, and decision tree modeling. The project will utilize a training and testing data set along with bagging. 

## Logistic Models

First we are going to start by building our logistic models, we are going to build three models and when we have all our models we are going to do some ROC analysis to pick the best models. For the three models we are going to do a full, reduced, and stepwise model and give a brief interpretation of them.

Starting with the full model, we also split the data set into our training and testing data to be utilized later. The full model included all the predictor variables available in the data set. Among these, Term, Internet_serviceNo Internet, Agreement_periodOne year contract, Agreement_periodTwo year contract, and Total_Charges were statistically significant, indicating that shorter customer terms, lack of internet service, shorter agreement periods, and higher total charges increased the likelihood of churn. Other variables, including gender, marital status, and phone/streaming plans, were not significant, suggesting they have little independent effect on churn after accounting for other factors. There were some factor levels with NAs because of multicollinearity however, keeping them in the model does not harm the model, it just means those factor levels can not be interpreted in context in a final model.  

```{r}
# Split data into training and testing
set.seed(123)
train_index <- sample(1:nrow(churn), 0.7 * nrow(churn))
train_data <- churn[train_index, ]
test_data <- churn[-train_index, ]

# Full logistic model including all variables
fullModel <- glm(Churn ~ Sex + Marital_Status + Term + Phone_service + International_plan + 
                   Voice_mail_plan + Multiple_line + Internet_service + Technical_support + 
                   Streaming_Videos + Agreement_period + Monthly_Charges + Total_Charges,
                 data = train_data, family = binomial)

summary(fullModel)


```

Next we look at our reduced model, which kept Term, Internet_service, Agreement_period, and Total_Charges as predictors. In this model significant predictors included Term, Internet_serviceNo Internet, Agreement_periodOne year contract, Agreement_periodTwo year contract, and Total_Charges. This model indicates that customers with shorter terms, no internet service, shorter agreement periods, and higher total charges are more likely to churn. Internet_serviceDSL showed marginal significance, while other levels of internet service were not significant.

```{r}
# Reduced logistic model
reducedModel <- glm(Churn ~ Term + Internet_service + Agreement_period + Total_Charges,
                    data = train_data, family = binomial)
summary(reducedModel)

```

Finally, we have our stepwise model, this model included Term, Technical_support, Agreement_period, Monthly_Charges, and Total_Charges. Significant predictors were Term, Technical_supportNo internet, Agreement_periodOne year contract, Agreement_periodTwo year contract, and Total_Charges. Somewhat significant predictors included Technical_supportYes and Monthly_Charges. The model suggests that shorter terms, lack of technical support, shorter agreement periods, and higher total charges increase the likelihood of churn, while other variables have little effect.

```{r}
# Stepwise logistic model (both directions)
stepModel <- stepAIC(fullModel, direction = "both", trace = FALSE)
summary(stepModel)

```

## Peceptron Models

Moving onto our peceptron model we will be looking at two models since peceptron does not have the natural ability to do a stepwise regression process like a logistic regression model would. Thus, for this analysis there will be a full and reduced model created. Before that happens though, the numeric variables need to undergo some scaling so they can be utilized properly in the peceptron models. 

```{r}
# Scale numeric variables
churn_nn <- churn %>%
  mutate(
    Term_scale = (Term - min(Term)) / (max(Term) - min(Term)),
    Monthly_Charges_scale = (Monthly_Charges - min(Monthly_Charges)) / 
                            (max(Monthly_Charges) - min(Monthly_Charges)),
    Total_Charges_scale = (Total_Charges - min(Total_Charges)) / 
                          (max(Total_Charges) - min(Total_Charges))
  )

# Drop original numeric columns
churn_nn <- churn_nn[, !(names(churn_nn) %in% c("Term", "Monthly_Charges", "Total_Charges"))]

# Convert Churn to numeric
churn_nn$Churn_num <- ifelse(churn_nn$Churn == "Yes", 1, 0)

# Convert categorical variables to dummy variables
cat_vars <- setdiff(colnames(churn_nn), c("Term_scale", "Monthly_Charges_scale", "Total_Charges_scale", "Churn", "Churn_num"))

churn_nn_dummy <- churn_nn %>%
  mutate(across(all_of(cat_vars), as.factor)) %>%
  model.matrix(~ . - 1, data = .) %>%
  as.data.frame()

# Clean column names
colnames(churn_nn_dummy) <- make.names(colnames(churn_nn_dummy))

# Add numeric target back
churn_nn_dummy$Churn_num <- churn_nn$Churn_num
```

After the necessary conversions and cleaning we can build and comment on our full model. The full model includes all predictors: sex, marital status, term, phone service, international plan, voice mail plan, multiple line status, internet service type, technical support, streaming videos, agreement period, monthly charges, and total charges. All numeric variables were scaled and categorical variables were converted to dummy variables. In this model, the weights indicate that streaming videos (No internet), monthly charges, and sex (Female) have relatively large positive influences on the hidden layer, while the connection from the hidden layer to the output shows a strong negative weight, reflecting the non-linear mapping learned for churn prediction. Overall, the model captures relationships across all features, although the influence of each feature varies.

```{r}
# Full Model

NN_vars_full <- setdiff(colnames(churn_nn_dummy), "Churn_num")
nn_formula_full <- as.formula(paste("Churn_num ~", paste(NN_vars_full, collapse = "+")))

NN_full <- neuralnet(nn_formula_full, data = churn_nn_dummy, hidden = 1,
                     act.fct = "logistic", linear.output = FALSE)

NN_full$result.matrix

```

The reduced model includes the most important predictors identified from prior logistic modeling: term, total charges, and agreement period (one-year and two-year contracts). In this simplified network, the hidden layer weights indicate strong positive and negative influences for term and total charges, with agreement period showing moderate positive influence. The weight from the hidden layer to the output is strongly negative, emphasizing the impact of these core predictors on churn probability. This reduced model highlights the most critical factors for churn while simplifying the network structure, making it easier to interpret compared to the full model.

```{r}

# Reduced Model
NN_vars_reduced <- c("Term_scale", "Total_Charges_scale", "Agreement_periodOne.year.contract", "Agreement_periodTwo.year.contract")
nn_formula_reduced <- as.formula(paste("Churn_num ~", paste(NN_vars_reduced, collapse = "+")))

NN_reduced <- neuralnet(nn_formula_reduced, data = churn_nn_dummy, hidden = 1,
                        act.fct = "logistic", linear.output = FALSE)

NN_reduced$result.matrix
```


## Decision Trees

For the decision tree model we are going to analyze three different options. The first will be a non-penalized model where there is no deduction for false negatives or false positives. Then we will be looking at a model that penalizes for false negatives, followed my a model that penalized for false positives. 

### Non-peanalized Model

This first model is a non-penalized model which means there are no deductions made when the model creates a false postie or negative result. Looking at this model breakdown the first split begins with the agreement period for the contract. The following split is on the monthly charges and whether or not those charges are under 95. After that the split is whether or not the term is greater than or equal to 12. Interestingly enough we then go back to a Monthly Charge split this time seeing if its under 44. Then we are followed by two Total Charge splits, the first one being if total charges are under 45 and then the next and final split being if charges are greater than or equal to 629. 

```{r}

tree.builder = function(in.data, fp, fn, purity){
   tree = rpart(Churn ~ .,                
                data = in.data, 
                na.action  = na.rpart,
                method = "class",
                model  = FALSE,
                x = FALSE,
                y = TRUE,
                parms = list(
                         loss = matrix(c(0, fp, fn, 0), ncol = 2, byrow = TRUE),
                         split = purity),
                control = rpart.control(
                        minsplit = 10,
                        minbucket = 10,
                        cp = 0.01,
                        xval = 10)
             )
}


```
 
 
```{r}
gini.tree.1.1 <- tree.builder(train_data, 1, 1, "gini")
rpart.plot(gini.tree.1.1, main = "Churn Tree: Gini Non-Penalized")


```

### Peanalized - False Negative

Now that the nonpenalized model has been created it is time to look at a peanalized model for false negatives. For this data set, a false negative would mean that the model incorrectly identifies a customer as a churn risk when they would actually stay loyal. This model also starts out with the split on the length of the Agreement Period. Then it has a left sided tree and a right sided tree. The left sided tree then splits on whether or not total charges are under 3933. Followed by a term greater than or equal to 58, then split on Multiple Lines, then gain on Term, then two total charge splits and an internet service split. Moving to the right tree it splits first on techincal support, followed by three total charges split, the first one is greater than or equal to 218, followed by a less than 132 split, and ending with a greater than or equal to 47 split. 

```{r}
# Penalized: FN = 10 times FP
gini.tree.1.10 <- tree.builder(train_data, 1, 10, "gini")

# Plot
rpart.plot(gini.tree.1.10, main = "Churn Tree: Gini Penalized (FN = 10x)")

```

### Peanalize - False Positive

Finishing out our model creation we are going to look at a penalizing model for false positives. For this data set, a false positive would mean that the model incorrectly identifies a customer as loyal when they actually leave. This model again splits on the agreement length. Followed by two monthly charges splits, the first one is for monthly charges under 95 and then its monthly charges greater than or equal to 101. The final split is for total charges greater than or equal to 2256. 

```{r}
# Penalized: FP = 10 times FN
gini.tree.10.1 <- tree.builder(train_data, 10, 1, "gini")

rpart.plot(gini.tree.10.1, main = "Churn Tree: Gini Penalized (FP = 10x)")


```
That is the third and final decision tree model we will be looking at for this data set. The next section will go into assessing both the candidate models from last weeks assignment and the three candidate models from above. This will allow us to determine the best candidate models for each of the three model types. 


## ROC Analysis

ROC Analysis and AUC will be used to determine which canidate models are the best from the models created over this week and last week. We are going to start with the logistic models, then look at the perceptron models, finishing with the decision trees. 

### Logistic Model

Looking at the results for our logistic models we can see that in terms of ranking the AUC output the stepwise model narrowly beats out the full model which narrowly beats out the reduced model. When selecting final models not only is AUC important but also making sure the model is reduced down if possible. Not only is the stepwise model smaller than the full model and equally sized to the reduced model, it has the highest AUC, which is ideal when selecting a final model. Taking all these factors into consideration, it makes sense to use the stepwise model as the final logistic regression model. 


```{r}

# --- Predictions for each logistic model ---
pred_full    <- predict(fullModel, newdata = test_data, type = "response")
pred_reduced <- predict(reducedModel, newdata = test_data, type = "response")
pred_step    <- predict(stepModel, newdata = test_data, type = "response")

# --- ROC and AUC for each ---
roc_full    <- roc(test_data$Churn, pred_full, levels = c("No", "Yes"))
roc_reduced <- roc(test_data$Churn, pred_reduced, levels = c("No", "Yes"))
roc_step    <- roc(test_data$Churn, pred_step, levels = c("No", "Yes"))

auc_full    <- auc(roc_full)
auc_reduced <- auc(roc_reduced)
auc_step    <- auc(roc_step)

# --- AUC summary table ---
AUC_logistic <- data.frame(
  Model = c("Full Logistic", "Reduced Logistic", "Stepwise Logistic"),
  AUC = c(auc_full, auc_reduced, auc_step)
)
AUC_logistic

plot(roc_full, col = "blue", lwd = 2, main = "ROC Curves for Logistic Models")
plot(roc_reduced, col = "red", lwd = 2, lty = 2, add = TRUE)
plot(roc_step, col = "darkgreen", lwd = 2, lty = 3, add = TRUE)
abline(0, 1, lty = 2, col = "gray")

legend("bottomright",
       legend = c(
         paste("Full (AUC =", round(auc_full, 3), ")"),
         paste("Reduced (AUC =", round(auc_reduced, 3), ")"),
         paste("Stepwise (AUC =", round(auc_step, 3), ")")
       ),
       col = c("blue", "red", "darkgreen"), lty = 1:3, lwd = 2, bty = "n")


```

### Perceptron Model

Moving onto the perceptron models we only have two to look at, the full and reduced model. When looking at this output we see something that is not common, a perfect 1.0 AUC and a right angle. While in typical scenarios a AUC of 1 is ideal, the likelihood of this actually happening is close to impossible. Some trouble shooting went on behind the scenes in an attemt to correct this and get a more accurate AUC but it was unsuccessful. It seems to be some kind of issue with data leakage that is unable to be fixed. Considering this unnatural AUC value and the full model being so complex it is typically not ideal, it makes sense not to move forward with this model. Thankfully, the reduced model does still have a good AUC value and it will work as the final perceptron model moving forward. 

```{r}
# Split churn_nn_dummy into train/test
set.seed(123)
train_index <- sample(1:nrow(churn_nn_dummy), 0.7 * nrow(churn_nn_dummy))
train_nn <- churn_nn_dummy[train_index, ]
test_nn  <- churn_nn_dummy[-train_index, ]

# Variables for full and reduced perceptron
NN_vars_full <- setdiff(colnames(train_nn), "Churn_num")
NN_vars_reduced <- c("Term_scale", "Total_Charges_scale", 
                     "Agreement_periodOne.year.contract", 
                     "Agreement_periodTwo.year.contract")

# Train full perceptron
NN_full <- neuralnet(as.formula(paste("Churn_num ~", paste(NN_vars_full, collapse="+"))),
                     data = train_nn, hidden = 1,
                     act.fct = "logistic", linear.output = FALSE)

# Train reduced perceptron
NN_reduced <- neuralnet(as.formula(paste("Churn_num ~", paste(NN_vars_reduced, collapse="+"))),
                        data = train_nn, hidden = 1,
                        act.fct = "logistic", linear.output = FALSE)

# Predictions on test set
NN_full_pred    <- compute(NN_full, test_nn[, NN_vars_full])$net.result
NN_reduced_pred <- compute(NN_reduced, test_nn[, NN_vars_reduced])$net.result

# ROC and AUC
roc_NN_full    <- roc(test_nn$Churn_num, NN_full_pred)
roc_NN_reduced <- roc(test_nn$Churn_num, NN_reduced_pred)

auc_NN_full    <- auc(roc_NN_full)
auc_NN_reduced <- auc(roc_NN_reduced)

# --- AUC summary table ---
AUC_perceptron <- data.frame(
  Model = c("Full Perceptron", "Reduced Perceptron"),
  AUC = c(auc_NN_full, auc_NN_reduced)
)
AUC_perceptron

# --- Plot ROC curves ---
plot(roc_NN_full, col = "purple", lwd = 2, main = "ROC Curves for Perceptron Models (Test Data)")
plot(roc_NN_reduced, col = "orange", lwd = 2, lty = 2, add = TRUE)
abline(0, 1, lty = 2, col = "gray")

legend("bottomright",
       legend = c(
         paste("Full (AUC =", round(auc_NN_full, 3), ")"),
         paste("Reduced (AUC =", round(auc_NN_reduced, 3), ")")
       ),
       col = c("purple", "orange"), lty = 1:2, lwd = 2, bty = "n")


```

### Decision Tree Model

Finally it is time to look at the decision tree models. This final model decision is slightly different because while we have 3 models we will be doing 6 ROC AUC comparisons. This is because each of the three models has a gini and info version. At their core the gini and info versions of a model are the same, they just use slightly different criteria when creating the tree. Looking at the output the model with the best AUC out of the 6 is the info false negative model, represented on the graph as info.1.10. This means the false negative model will be the final decision tree model for the future analysis. 

```{r}
SenSpe <- function(in.data, fp, fn, purity){
  cutoff = seq(0, 1, length = 20)
  model = tree.builder(in.data, fp, fn, purity)
  pred = predict(model, newdata = in.data, type = "prob")
  
  # Initialize matrix for Sensitivity and Specificity
  senspe.mtx = matrix(0, ncol = length(cutoff), nrow = 2)
  
  for (i in 1:length(cutoff)){
    pred.out = ifelse(pred[,"Yes"] >= cutoff[i], "Yes", "No")
    TP = sum(pred.out == "Yes" & in.data$Churn == "Yes")
    TN = sum(pred.out == "No"  & in.data$Churn == "No")
    FP = sum(pred.out == "Yes" & in.data$Churn == "No")
    FN = sum(pred.out == "No"  & in.data$Churn == "Yes")
    
    senspe.mtx[1,i] = TP / (TP + FN)  # Sensitivity
    senspe.mtx[2,i] = TN / (TN + FP)  # Specificity
  }
  
  # ROC and AUC calculation
  ROCobj <- roc(in.data$Churn == "Yes", pred[,"Yes"])
  AUC = auc(ROCobj)
  
  list(senspe.mtx = senspe.mtx, AUC = round(AUC, 3))
}

```


```{r}
# Non-penalized
giniROC11   = SenSpe(in.data = train_data, fp = 1, fn = 1, purity = "gini")
infoROC11   = SenSpe(in.data = train_data, fp = 1, fn = 1, purity = "information")

# Penalize false negatives (customers leaving)
giniROC110  = SenSpe(in.data = train_data, fp = 1, fn = 10, purity = "gini")
infoROC110  = SenSpe(in.data = train_data, fp = 1, fn = 10, purity = "information")

# Penalize false positives (customers incorrectly flagged as leaving)
giniROC101  = SenSpe(in.data = train_data, fp = 10, fn = 1, purity = "gini")
infoROC101  = SenSpe(in.data = train_data, fp = 10, fn = 1, purity = "information")


```


```{r}

par(pty = "s")  # square plot
colors = c("#008B8B", "#00008B", "#8B008B", "#8B0000", "#8B8B00", "#8B4500")

plot(1 - giniROC11$senspe.mtx[2,], giniROC11$senspe.mtx[1,], 
     type = "l", 
     xlim = c(0, 1), 
     ylim = c(0, 1), 
     xlab = "1 - Specificity: FPR", 
     ylab = "Sensitivity: TPR", 
     col = colors[1], 
     lwd = 2,
     main = "ROC Curves of Decision Trees", 
     cex.main = 0.9, 
     col.main = "navy")

abline(0, 1, lty = 2, col = "orchid4", lwd = 2)

lines(1 - infoROC11$senspe.mtx[2,], infoROC11$senspe.mtx[1,], 
      col = colors[2], lwd = 2, lty = 2)
lines(1 - giniROC110$senspe.mtx[2,], giniROC110$senspe.mtx[1,],
      col = colors[3], lwd = 2)
lines(1 - infoROC110$senspe.mtx[2,], infoROC110$senspe.mtx[1,], 
      col = colors[4], lwd = 2, lty = 2)
lines(1 - giniROC101$senspe.mtx[2,], giniROC101$senspe.mtx[1,], 
      col = colors[5], lwd = 2, lty = 4)
lines(1 - infoROC101$senspe.mtx[2,], infoROC101$senspe.mtx[1,], 
      col = colors[6], lwd = 2, lty = 2)

legend("bottomright",
       c(paste("gini.1.1,  AUC =", giniROC11$AUC), 
         paste("info.1.1,  AUC =", infoROC11$AUC), 
         paste("gini.1.10, AUC =", giniROC110$AUC), 
         paste("info.1.10, AUC =", infoROC110$AUC),
         paste("gini.10.1, AUC =", giniROC101$AUC), 
         paste("info.10.1, AUC =", infoROC101$AUC)),
       col = colors, 
       lty = rep(1:2, 3), 
       lwd = rep(2, 6), 
       cex = 0.8, 
       bty = "n")

```

```{r}

# Create AUC summary table
AUC_results <- data.frame(
  Model = c("Gini (1,1)", "Gini (1,10)", "Gini (10,1)",
            "Info (1,1)", "Info (1,10)", "Info (10,1)"),
  AUC = c(giniROC11$AUC, giniROC110$AUC, giniROC101$AUC,
          infoROC11$AUC, infoROC110$AUC, infoROC101$AUC)
)

AUC_results

```

The selection of each model types final and optimal model ends the ROC analysis. In order to determine the overall best model from the three final models the bagging AUC method will be used, which will be done in the next section. 

## Bagging AUC

Now that the final model from each section has been selected it is time to compare these three models to each other to pick the optimal final model. To do this the bagging method will be utilized. Bagging will be run on each of the three models and a distribution of the different AUCs will be outputted. In addition the output will also include a 95% confidence interval for the AUCs and the mean AUC for each model. All this output will be analyzed to determine the optimal final model. 

Starting with the sampling distribution histograms is appears that the perceptron model has the lowest AUC distribution of the three models. The decision tree and logistic models have similar distributions so further output will need to be analyzed to differentiate them when selecting the best model. For this differentiation the 95% confidence intervals and the mean AUC outputs will be utilized. For the logistic stepwise model there is a mean AUC on 0.8439 and a 95% CI of [0.81122397, 0.8736398]. Then for the decision tree model there is a mean AUC of 0.8444 and a 95% CI of [0.8043273, 0.8790614]. At first glance it may appear that the decision tree model is the best model considering its higher AUC. However, it should be noted that the logistic stepwise model has a very similar AUC. Additionally, the logistic stepwise model has a smaller confidence interval than the decision tree model, and a smaller confidence interval is ideal. Either model could be selected as the final model, and that decision will be discussed in the final results and conclusion. 

```{r}

set.seed(123)

# Number of bootstrap resamples
B <- 1000

# Function to perform bootstrap AUC evaluation
bootstrap_auc <- function(model, data, model_type, model_name = "Model", response_col = "Churn") {
  auc_vec <- numeric(B)
  sample.size <- nrow(data)
  
  for (k in 1:B) {
    boot.id <- sample(1:sample.size, sample.size, replace = TRUE)
    boot.sample <- data[boot.id, ]
    
    if (model_type == "logistic") {
      preds <- predict(model, newdata = boot.sample, type = "response")
      
    } else if (model_type == "tree") {
      preds <- predict(model, newdata = boot.sample, type = "prob")[, "Yes"]
      
    } else if (model_type == "nn") {
      # Preprocessing to match NN training
      boot.sample$Term_scale <- scale(boot.sample$Term)
      boot.sample$Total_Charges_scale <- scale(boot.sample$Total_Charges)
      boot.sample$Agreement_periodOne.year.contract <- ifelse(boot.sample$Agreement_period == "One year contract", 1, 0)
      boot.sample$Agreement_periodTwo.year.contract <- ifelse(boot.sample$Agreement_period == "Two year contract", 1, 0)
      
      newdata <- boot.sample[, c("Term_scale",
                                 "Total_Charges_scale",
                                 "Agreement_periodOne.year.contract",
                                 "Agreement_periodTwo.year.contract"), drop = FALSE]
      preds <- compute(model, newdata)$net.result
      preds <- as.vector(preds)
    }
    
    ROCobj <- roc(boot.sample[[response_col]], preds, levels = c("No", "Yes"), quiet = TRUE)
    auc_vec[k] <- as.numeric(auc(ROCobj))
  }
  
  # Print mean AUC with model name
  cat(paste0("Mean Bootstrap AUC for ", model_name, ": ", round(mean(auc_vec), 4), "\n"))
  
  return(auc_vec)
}

# Run bootstrapping for each final model
btAUC_logistic <- bootstrap_auc(stepModel, train_data, model_type = "logistic", model_name = "Logistic Stepwise")
btAUC_nn       <- bootstrap_auc(NN_reduced, train_data, model_type = "nn", model_name = "Reduced Perceptron")
btAUC_tree     <- bootstrap_auc(gini.tree.1.1, train_data, model_type = "tree", model_name = "Decision Tree FN Info")
# Combine results into a data frame for plotting
boot_df <- data.frame(
  AUC = c(btAUC_logistic, btAUC_nn, btAUC_tree),
  Model = factor(rep(c("Stepwise Logistic", "Reduced Perceptron", "Decision Tree FN"),
                     each = B))
)

# Plot AUC distributions
ggplot(boot_df, aes(x = AUC, fill = Model)) +
  geom_histogram(alpha = 0.6, position = "identity", bins = 30) +
  facet_wrap(~Model, scales = "free_y") +
  theme_minimal(base_size = 13) +
  labs(title = "Bootstrap Sampling Distributions of AUCs (Final Models)",
       x = "AUC", y = "Frequency") +
  theme(legend.position = "none")

# Compute 95% Confidence Intervals
logistic_ci <- quantile(btAUC_logistic, c(0.025, 0.975))
nn_ci       <- quantile(btAUC_nn, c(0.025, 0.975))
tree_ci     <- quantile(btAUC_tree, c(0.025, 0.975))

cat("95% Bootstrap CI for Stepwise Logistic:", logistic_ci, "\n")
cat("95% Bootstrap CI for Reduced Perceptron:", nn_ci, "\n")
cat("95% Bootstrap CI for Decision Tree FN", tree_ci, "\n")

```

# Results & Conclusion

After understanding and analyzing the three final models, looking at their ROC and bagging AUC output, my determination that the optimal final model is the logistic stepwise model. While it has a slightly smaller AUC than the decision tree model, the decision tree model has a wider confidence interval. Additionally, in a more practical sense logistic models are know for having less variance than decision tree models, and are a more stable model to work with. As a reminder, here is a contextualized explanation of that model. For every 1-unit increase in Term, the log-odds of churn decrease by 0.0758, meaning the odds of churn are multiplied by approximately 0.93 (a 7% decrease). Customers with no internet have log-odds of churn that are 1.309 lower than the reference group, while those with technical support have log-odds that are 0.451 lower. Having a one-year contract reduces the log-odds of churn by 1.612, and having a two-year contract reduces them by 2.146, compared to customers on month-to-month agreements. For each 1-unit increase in Monthly_Charges, the log-odds of churn increase by 0.0118, and for each 1-unit increase in Total_Charges, the log-odds of churn increase by 0.00064. Looking at this final optimal model in comparison to other models there appears to be a good amount of variable overlap, which strengthens our support in the significance of these variables, and this being a strong optimal model. 


# General Discussions

It should be noted that no project is without fault or could result in different conclusions with added context. As discussed earlier the logistic and decision tree models were very similar in the final analysis and there were arguments for both to be the final model. The decision was ultimately left in my hands. However, this model is not being presented to anyone without a background and understanding in statistics. If this was a scenario where I was presenting this to the telecommunication companies and I knew they had very limited statistical knowledge I most likely would have selected the decision tree model. While it has that slightly wider confidence interval it is more straightforward and interpretable than a log odds stepwise model for those with limited statistical knowledge. If a model is being created for non statisticians to use and understand sometimes a slightly weaker straightforward model is ultimately better than a slightly stronger complex model. This is because if the people you create the model for do not understand it or how to properly implement the model, it is utterly useless to them. However, since there is no scenario tied to this report it was my decision to utilize the model I identified as stronger despite being more complex for a non statistician. 
