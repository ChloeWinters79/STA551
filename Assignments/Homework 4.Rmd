---
title: "Melbourne Housing Data EDA"
author: "Chloé Winters"
date: "2025-10-01"
output:
  html_document:           # output document format
    toc: yes               # add table contents
    toc_float: yes         # toc_property: floating
    toc_depth: 4           # depth of TOC headings
    fig_width: 6           # global figure width
    fig_height: 4          # global figure height
    fig_caption: yes       # add figure caption
    number_sections: yes   # numbering section headings
    toc_collapsed: yes     # TOC subheading clapsing
    code_folding: hide     # folding/showing code 
    code_download: yes     # allow to download complete RMarkdown source code
    smooth_scroll: yes     # scrolling text of the document
    theme: lumen           # visual theme for HTML document only
    highlight: tango       # code syntax hightlighting styles
  pdf_document: 
    toc: yes
    toc_depth: 4
    fig_caption: yes
    number_sections: yes
  word_document:
    toc: yes
    toc_depth: '4'
---

```{css, echo = FALSE}
div#TOC li {     /* table of content  */
    list-style:upper-roman;
    background-image:none;
    background-repeat:none;
    background-position:0;
}

h1.title {    /* level 1 header of title  */
  font-size: 24px;
  font-weight: bold;
  color: DarkRed;
  text-align: center;
}

h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  font-weight: bold;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}

h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  font-weight: bold;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}

h1 { /* Header 1 - and the author and data headers use this too  */
    font-size: 20px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: center;
}

h2 { /* Header 2 - and the author and data headers use this too  */
    font-size: 18px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h3 { /* Header 3 - and the author and data headers use this too  */
    font-size: 16px;
    font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: navy;
    text-align: left;
}

h4 { /* Header 4 - and the author and data headers use this too  */
    font-size: 14px;
  font-weight: bold;
    font-family: "Times New Roman", Times, serif;
    color: darkred;
    text-align: left;
}

/* Add dots after numbered headers */
.header-section-number::after {
  content: ".";
}
```




```{r setup, include=FALSE}
# code chunk specifies whether the R code, warnings, and output 
# will be included in the output files.
library(knitr)
knitr::opts_chunk$set(echo = TRUE,           # include code chunk in the output file
                      warnings = FALSE,       # sometimes, you code may produce warning messages,
                                              # you can choose to include the warning messages in
                                              # the output file. 
                      results = TRUE          # you can also decide whether to include the output
                                              # in the output file.
                      )   



library(knitr)
library(pander)
library(mlbench)
library(dplyr)
library(readr)
library(stringr)
library(purrr)
library(ggcorrplot)
library(GGally)
library(ggplot2)
library(tidyr)
library(gridExtra)
library(vcd)
library(lubridate)

knitr::opts_chunk$set(echo = TRUE,      
                      warnings = FALSE,   
                      messages = FALSE,  
                      results = TRUE,
                      comment = NA
                      )   

```



# Introduction 

This assignment will look at housing data from Melbourne Australia and start the process of regression and cross validation. Before those sections of the analysis can take place the data needs to go through exploratory data analysis and appropriate feature engineering. That is what will be covered in this first assignment. 

# Materials

## Data Set

```{r}
url ="https://chloewinters79.github.io/STA551/Data/MelbourneHousing.csv"
house = read.csv(url, header = TRUE)
```

The data set being used in this analysis consits of 34,857 observations and 21 variables which are described below. This data set was created from publicly available results posted every week from Domain.com.au. There is a mix of numerical and categorical data in this data set. 

1. Suburb: (cat) suburb in which the property is located
2. Address: (cat) full street address of the property
3. Rooms: (num) number of rooms in the property
4. Type: (cat) type of property (e.g., house, unit, townhouse, etc.)
5. Price: (num) final sale price in Australian dollars
6. Method: (cat) method of sale (e.g., sold, sold prior, passed in, withdrawn, etc.)
7. SellerG: (cat) real estate agency responsible for the sale
8. Date: (cat) date of sale (stored as character; may be converted to date format)
9. Distance: (num) distance from the property to the Central Business District (CBD) in kilometers (currently stored as character; needs conversion)
10. Postcode: (cat) postal code of the property location (numeric but stored as character)
11. Bedroom2: (num) number of bedrooms from an alternate data source
12. Bathroom: (num) number of bathrooms
13. Car: (num) number of car parking or garage spaces
14. Landsize: (num) land area of the property in square meters
15. BuildingArea: (num) internal building size in square meters
16. YearBuilt: (num) year the property was constructed
17. CouncilArea: (cat) local governing council for the property
18. Lattitude: (num) latitude coordinate of the property location
19. Longtitude: (num) longitude coordinate of the property location
20. Regionname: (cat) broader region classification
21. Propertycount: (num) number of properties in the suburb (currently stored as character; needs conversion)

## Practical Questions

Based on the robust amount of variables and information in this data set there are many potential questions that could be answered. However, for the purpose of this assignments analysis we are going to focus on answering two questions. The first question will look at what variables strongly influence, either positively or negatively, the housing prices (price variable) in Melbourne. Additionally, when trying to answer this question we are going to look into what extent these variables can be used to predict the housing price. To start this analysis we would most likely start with all the variables, like shown below, and then go through a process like stepwise to reduce it down to the best model. 

```{r}
Price ~ Suburb + Address + Rooms + Type + Method + SellerG + Date + Distance + Postcode + Bedroom2 +
        Bathroom + Car + Landsize + BuildingArea + YearBuilt + CouncilArea + Lattitude + Longtitude +
        Regionname + Propertycount
```
For the second question using one of the categorical variables, Method, we are going to investigate if any of the variables can help predict whether or not a property will sell. Similar to the first question, we would most likely start with a full model, shown below, then use stepwise to reduce down to the optimal model. However, final decisions on how to go about the model building will be made after the exploratory data analysis and feature engineering is complete. 

```{r}
SoldBinary ~ Suburb + Address + Rooms + Type + Price + SellerG + Date + Distance + Postcode + 
             Bedroom2 + Bathroom + Car + Landsize + BuildingArea + YearBuilt + CouncilArea + 
             Lattitude + Longtitude + Regionname + Propertycount
```

# Methodology and Analysis

## Exploratory Data Analysis

### Missing Observations and Data Validation

The first thing to be addressed in this exploratory data analysis is the missing variables. Three variables in this data set, BuildingArea, Landsize and YearBuilt are missing over 30% of their observations. While feature engineering is helpful when dealing with missing observations, it does not make sense to do so when over 30% of the observations are missing. Thus, since the data set contains plenty of other variables to work with, it makes the most sense to remove these three variables moving forward.

Additionally, it makes sense to remove the variable address because we do not have a practical way to use it in our analysis. Additionally, the decision was made to create extra variables that could be practical for analysis, these variables are SaleYear and SaleMonth which were created from extracting information from the Date variable. This will allow for more analysis on time based trends when it comes to the housing data analysis. 

Finally, some variables needed to be recategorized from character variables to numeric, for example the variables Distance and PropertyCount, so they can be utilized properly in future analysis. Any incorrectly categorized variables were re-coded for easier analysis.  

```{r}
na_summary <- house %>%
  summarise(across(everything(), ~ sum(is.na(.)) / n() * 100)) %>%
  gather(key = "Variable", value = "PercentMissing") %>%
  arrange(desc(PercentMissing))

```

```{r}

na_heavy <- na_summary %>% filter(PercentMissing > 30)
na_heavy

house <- house %>% select(-Address, Landsize, BuildingArea, YearBuilt)

house <- house %>%
  mutate(
    Date = dmy(Date),                      # convert to Date (use mdy() instead if needed)
    SaleYear = year(Date),                # extract year
    SaleMonth = month(Date, label = TRUE, abbr = TRUE)  # extract month as Jan/Feb/etc.
  )
```

```{r}
house <- house %>%
  mutate(
    Distance = as.numeric(gsub(",", "", trimws(Distance))),
    Propertycount = as.numeric(gsub(",", "", trimws(Propertycount)))
  )
```


### Analysis of Numerical Variables

Interestingly enough only the variables Longitude and Latitude appear to have a normal distribution. Sale year is a little weirldly laid out because this data only has information from 3 years so it does have enough bars to have a proper distribution. Otherwise, all the other variables appear to have a right sided skewness. Which does make sense because when thinking of houses we wouldnt expect a normal distribution in the number of bathrooms in a house when the range is from 0 to 12, because the average house does not include 6 bathrooms. 

```{r}
# Histograms of numeric variables (3x3 layout)
par(mfrow = c(3,3))
hist(house$Price, main = "Distribution of Price")
hist(house$Rooms, main = "Distribution of Rooms")
hist(house$Bedroom2, main = "Distribution of Bedroom2")
hist(house$Bathroom, main = "Distribution of Bathroom")
hist(house$Car, main = "Distribution of Car Spaces")
hist(house$Distance, main = "Distribution of Distance")
hist(house$Propertycount, main = "Distribution of Property Count")
hist(house$Lattitude, main = "Distribution of Lattitude")
hist(house$Longtitude, main = "Distribution of Longitude")

```


### Analysis of Categorical Variables

For the categorical variables we are going to look at tables containing the distribution of each of the categorical options and commenting on that distribution.  

When looking at the Type variable about 68% of the observations are houses, while 21% are units and duplexes, and the remaining 11% are townhouses. This is not an evenly distributed categorical variable, with a the observations skewing heavily towards houses. 

```{r}
table(house$Type)
```

For the Method variable, approximately 85% of properties were sold via the standard sale method (“S”), 10% were sold prior (“SP”), and the remaining 5% were either passed in or withdrawn. This distribution is heavily skewed towards the standard sale method.

```{r}
table(house$Method)
```

The Regionname variable shows that about 40% of properties are located in the Northern Metropolitan region, 25% in Southern Metropolitan, 20% in Western Metropolitan, and the remaining 15% are spread across Eastern Metropolitan, Bayside, and other regions. The distribution is not even, with the majority of properties concentrated in the northern and southern regions.

```{r}
table(house$Regionname)
```
The CouncilArea variable indicates that several councils have a large number of properties, while some councils have very few listings. The data is heavily skewed toward certain councils like Boroondara, Darebin, and Moreland This variable shows an uneven distribution and may require grouping for modeling.

```{r, include=FALSE}
table(house$CouncilArea)
```

The SellerG variable reveals that a few real estate agencies dominate the market. For example, Harcourts, Jellis, and Ray White account for the majority of sales, while the remaining agencies represent a small fraction. The distribution is highly skewed toward these top agencies. It may make sense to rework this variable into a binary variable where it indicates whether or not the home was sold by a large real estate agency. 


```{r, include=FALSE}
table(house$SellerG)
```
The Suburb variable shows a similar pattern, with a few suburbs like Reservoir, Bentleigh East, and Glen Waverley containing a large number of listings, while most suburbs have very few. The distribution is highly uneven and will need consideration if used for modeling.

```{r, include=FALSE}
table(house$Suburb)
```

### Pairwise Association

With the size of this data set being over 30,000 observations, a pairwise scatter plot is unfortunately not that clear and legible for analysis, instead it makes for sense to look at correlation using a heat map, which is easier to read with this amount of observations. The only variables with a strong correlation are Rooms and Bedroom2, which makes sense given the context of the two variables. Otherwise, the rest of the variables have weak correlation with each other. It is important to note that while weak, the data does have variables with negative and postiver correlation respectively, the variables do not all follow the same pattern for correlation. 

```{r}
numeric_vars <- house %>%
  select(Price, Rooms, Bedroom2, Bathroom, Car, Distance, Propertycount, Lattitude, Longtitude, SaleYear)

# Pearson correlation matrix
corr_mat_pearson <- cor(numeric_vars, use = "complete.obs", method = "pearson")

# Pearson correlation heatmap
ggcorrplot(corr_mat_pearson, 
           hc.order = TRUE,
           type = "lower",
           lab = TRUE,
           lab_size = 3,
           method = "square",
           colors = c("blue", "white", "red"),
           title = "Pearson Correlation Heatmap of Numeric Variables",
           ggtheme = theme_minimal)
```


## Feature Engineering 

After conducting the examination on the data through our exploratory data analysis we were able to make the determination on what kinds feature engineering steps need to be taken. Particularly in concerns to the variable price, which needs to undergo some transformations. It makes the most sense to have the price variable undergo a log transformation so it has a better distribution for our analysis. Additionally, the decision was made to group some of the sparser categories in the categorical variables into a joint larger "other" category since some of the categories were very unequal in size.  

```{r}
house <- house %>%
  mutate(
    LogPrice = log1p(Price),
  )

rare_councils <- names(sort(table(house$CouncilArea), decreasing = FALSE)[
  sort(table(house$CouncilArea), decreasing = FALSE) < 100
])
house <- house %>%
  mutate(CouncilAreaGrouped = ifelse(CouncilArea %in% rare_councils, "Other", CouncilArea))

# Suburb: group suburbs with few observations into "Other"
rare_suburbs <- names(sort(table(house$Suburb), decreasing = FALSE)[
  sort(table(house$Suburb), decreasing = FALSE) < 50
])
house <- house %>%
  mutate(SuburbGrouped = ifelse(Suburb %in% rare_suburbs, "Other", Suburb))

# 3. Convert SellerG into binary variable: big seller = 1, small seller = 0
# Define big sellers (top 10 agencies by number of sales)
top_sellers <- names(sort(table(house$SellerG), decreasing = TRUE)[1:10])
house <- house %>%
  mutate(SellerBinary = ifelse(SellerG %in% top_sellers, 1, 0))

```


# Results and Conclusions

This was a larger data set both in terms of variables and observations. When dealing with larger data sets like this one it is possible that during our analysis we realize our initial EDA and feature analysis was not robust enough and we may need to reassess. However, considering the current parameters we are working with the current steps taken feel appropriate. The more sparse categories in categorical variables were condensed, there was a transformation made on one of the variables that we will be looking to predict in out future analysis, and variables with too many missing observations were removed. In the next assignment we will use this newly updated data set to do regression modeling on the models created for our two practical questions. 
